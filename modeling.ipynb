{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VEKTORIZACIJA\n",
    "\n",
    "# TF-IDF\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tf_idf(X):\n",
    "    tfidf_vectorizer = TfidfVectorizer(norm='l2', max_features=200)\n",
    "    tfidf_vectorizer.fit(X)\n",
    "    tfidf_vectors = tfidf_vectorizer.transform(X)\n",
    "    return tfidf_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF atveju stemingas ir lemavimas (reikalingi failai gauti iš preprocess.ipynb):\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "stemming_df = pd.read_csv(\"cleaned_data/stemming.csv\")\n",
    "lemmatization_df = pd.read_csv(\"cleaned_data/lemmatization.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>GENRE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['listen', 'convers', 'doctor', 'parent', '10y...</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['brother', 'sister', 'past', 'incestu', 'rela...</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['bu', 'empti', 'student', 'field', 'trip', 'm...</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['help', 'unemploy', 'father', 'make', 'end', ...</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['film', 'titl', 'refer', 'unrecov', 'bodi', '...</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54081</th>\n",
       "      <td>['shortliv', 'nbc', 'live', 'sitcom', 'center'...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54082</th>\n",
       "      <td>['next', 'gener', 'exploit', 'sister', 'kapa',...</td>\n",
       "      <td>horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54083</th>\n",
       "      <td>['ze', 'bestaan', 'echt', 'standup', 'comedi',...</td>\n",
       "      <td>documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54084</th>\n",
       "      <td>['walter', 'vivian', 'live', 'countri', 'diffi...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54085</th>\n",
       "      <td>['labor', 'day', 'weekend', '1935', 'intens', ...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54086 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             DESCRIPTION          GENRE\n",
       "0      ['listen', 'convers', 'doctor', 'parent', '10y...         drama \n",
       "1      ['brother', 'sister', 'past', 'incestu', 'rela...      thriller \n",
       "2      ['bu', 'empti', 'student', 'field', 'trip', 'm...         adult \n",
       "3      ['help', 'unemploy', 'father', 'make', 'end', ...         drama \n",
       "4      ['film', 'titl', 'refer', 'unrecov', 'bodi', '...         drama \n",
       "...                                                  ...            ...\n",
       "54081  ['shortliv', 'nbc', 'live', 'sitcom', 'center'...        comedy \n",
       "54082  ['next', 'gener', 'exploit', 'sister', 'kapa',...        horror \n",
       "54083  ['ze', 'bestaan', 'echt', 'standup', 'comedi',...   documentary \n",
       "54084  ['walter', 'vivian', 'live', 'countri', 'diffi...        comedy \n",
       "54085  ['labor', 'day', 'weekend', '1935', 'intens', ...       history \n",
       "\n",
       "[54086 rows x 2 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemming_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DESCRIPTION</th>\n",
       "      <th>GENRE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['listening', 'conversation', 'doctor', 'paren...</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['brother', 'sister', 'past', 'incestuous', 'r...</td>\n",
       "      <td>thriller</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['bus', 'empty', 'student', 'field', 'trip', '...</td>\n",
       "      <td>adult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['help', 'unemployed', 'father', 'make', 'end'...</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['film', 'title', 'refers', 'unrecovered', 'bo...</td>\n",
       "      <td>drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54081</th>\n",
       "      <td>['shortlived', 'nbc', 'live', 'sitcom', 'cente...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54082</th>\n",
       "      <td>['next', 'generation', 'exploitation', 'sister...</td>\n",
       "      <td>horror</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54083</th>\n",
       "      <td>['ze', 'bestaan', 'echt', 'standup', 'comedy',...</td>\n",
       "      <td>documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54084</th>\n",
       "      <td>['walter', 'vivian', 'live', 'country', 'diffi...</td>\n",
       "      <td>comedy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54085</th>\n",
       "      <td>['labor', 'day', 'weekend', '1935', 'intense',...</td>\n",
       "      <td>history</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>54086 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             DESCRIPTION          GENRE\n",
       "0      ['listening', 'conversation', 'doctor', 'paren...         drama \n",
       "1      ['brother', 'sister', 'past', 'incestuous', 'r...      thriller \n",
       "2      ['bus', 'empty', 'student', 'field', 'trip', '...         adult \n",
       "3      ['help', 'unemployed', 'father', 'make', 'end'...         drama \n",
       "4      ['film', 'title', 'refers', 'unrecovered', 'bo...         drama \n",
       "...                                                  ...            ...\n",
       "54081  ['shortlived', 'nbc', 'live', 'sitcom', 'cente...        comedy \n",
       "54082  ['next', 'generation', 'exploitation', 'sister...        horror \n",
       "54083  ['ze', 'bestaan', 'echt', 'standup', 'comedy',...   documentary \n",
       "54084  ['walter', 'vivian', 'live', 'country', 'diffi...        comedy \n",
       "54085  ['labor', 'day', 'weekend', '1935', 'intense',...       history \n",
       "\n",
       "[54086 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatization_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectors_stemming = tf_idf(stemming_df['DESCRIPTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vectors_lemmatization = tf_idf(lemmatization_df[\"DESCRIPTION\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54086, 200)\n",
      "727501\n"
     ]
    }
   ],
   "source": [
    "# Stemingo atveju\n",
    "\n",
    "print(tf_idf_vectors_stemming.toarray().shape) # Kokio dydzio matrica sukure?\n",
    "print(tf_idf_vectors_stemming.nnz) # Skaiciu, kuriu yra ne nulis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54086, 200)\n",
      "650859\n"
     ]
    }
   ],
   "source": [
    "# Lematizavimo atveju\n",
    "\n",
    "print(tf_idf_vectors_lemmatization.toarray().shape)\n",
    "print(tf_idf_vectors_lemmatization.nnz) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tekstas vektorizuotas. CNN atveju tikėsis 'array' tipo X, bei y reik konvertuot į atitinkamas one-hot encoded reikšmes\n",
    "Kintamųjų pavadinimai toliau sutrumpinti: lem - lemmatization, stem - stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "X_lem = tf_idf_vectors_lemmatization.toarray()\n",
    "y_lem = onehot_encoder.fit_transform(lemmatization_df[\"GENRE\"].values.reshape(-1, 1))\n",
    "\n",
    "X_stem = tf_idf_vectors_stemming.toarray()\n",
    "y_stem = onehot_encoder.fit_transform(stemming_df[\"GENRE\"].values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ar žanrai taip pat užkoduoti: True\n"
     ]
    }
   ],
   "source": [
    "# Is tikruju zanrai siuo atveju turetu but taip pat užkoduoti, bet patikrinu:\n",
    "import numpy as np\n",
    "are_equal = np.array_equal(y_stem, y_lem)\n",
    "print(f\"Ar žanrai taip pat užkoduoti: {are_equal}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dabar turime lematizuotus ir steminguotus vektorius X, bei žanrų masyvus y, paruoštus modelio mokymui."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEMATIZAVIMO atveju:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pakeičiama forma TF-IDF vektorių į 3D, nes to tikėsis CNN\n",
    "X_lem_reshaped = X_lem.reshape((X_lem.shape[0], X_lem.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_lem_train, X_lem_test, y_lem_train, y_lem_test = train_test_split(X_lem_reshaped, y_lem, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input\n",
    "\n",
    "# Apsirašomas CNN modelis\n",
    "model_lem_tftidf_cnn  = Sequential([\n",
    "    Input(shape=(X_lem_train.shape[1], 1)), \n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_lem_train.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Sukompiliuojamas modelis\n",
    "model_lem_tftidf_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Apsirasomas ankstyvas mokymosi sustabdymas.\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Galima naudot ir 'val_accuracy'\n",
    "    patience=3,  # Kai tiek epochų modelis nebetobuleja - bus sustabdytas mokymasis\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 23ms/step - accuracy: 0.3577 - loss: 2.2806 - val_accuracy: 0.4219 - val_loss: 1.9784\n",
      "Epoch 2/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4125 - loss: 2.0257 - val_accuracy: 0.4312 - val_loss: 1.9249\n",
      "Epoch 3/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4217 - loss: 1.9765 - val_accuracy: 0.4288 - val_loss: 1.9120\n",
      "Epoch 4/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4186 - loss: 1.9628 - val_accuracy: 0.4366 - val_loss: 1.9122\n",
      "Epoch 5/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 24ms/step - accuracy: 0.4236 - loss: 1.9490 - val_accuracy: 0.4381 - val_loss: 1.8871\n",
      "Epoch 6/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4302 - loss: 1.9215 - val_accuracy: 0.4369 - val_loss: 1.8866\n",
      "Epoch 7/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4300 - loss: 1.9288 - val_accuracy: 0.4356 - val_loss: 1.8781\n",
      "Epoch 8/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 24ms/step - accuracy: 0.4303 - loss: 1.9138 - val_accuracy: 0.4324 - val_loss: 1.8767\n",
      "Epoch 9/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4394 - loss: 1.8984 - val_accuracy: 0.4373 - val_loss: 1.8766\n",
      "Epoch 10/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4304 - loss: 1.9085 - val_accuracy: 0.4349 - val_loss: 1.8775\n"
     ]
    }
   ],
   "source": [
    "# Modelis mokosi apie 4 min\n",
    "history_lem_tfidf_cnn = model_lem_tftidf_cnn.fit(X_lem_train, y_lem_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kad nereiktu kiekviena kart leisti modelio mokymosi (auksciau esanciam bloke), galima ji issaugoti:\n",
    "# Turi but uztikrinta kad tame failo direktorijoje yra aplankalas 'models'\n",
    "model_lem_tftidf_cnn.save('models/lem_tfidf_cnn.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pavizdys, kaip reiketu apmokyta modeli uzkrauti\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_lem_tftidf_cnn = load_model('models/lem_tfidf_cnn.keras')\n",
    "model_lem_tftidf_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy']) # iš naujo kompiliuojame su optimizatoriumi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "Lemavimas + TF-IDF + CNN\n",
      "F1 Score: 0.354\n",
      "Tikslumas: 0.443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Spetos etiketes (skaiciukai atitikantys zanrus)\n",
    "y_pred = model_lem_tftidf_cnn.predict(X_lem_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "y_true_classes = y_lem_test.argmax(axis=1) \n",
    "\n",
    "accuracy_f1_lem_tfidf_cnn = round(accuracy_score(y_true_classes, y_pred_classes), 3)\n",
    "f1_lem_tfidf_cnn = round(f1_score(y_true_classes, y_pred_classes, average='weighted'), 3)\n",
    "\n",
    "print(\"Lemavimas + TF-IDF + CNN\")\n",
    "print(f\"F1 Score: {f1_lem_tfidf_cnn}\")\n",
    "print(f\"Tikslumas: {accuracy_f1_lem_tfidf_cnn}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tai Lemavimo + TF-IDF + CNN atveju gauname:\n",
    "Tikslumas: 0.443\n",
    "F1: 0.354"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toliau stemingas + TF-IDF + CNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Pakeičiama forma TF-IDF vektorių į 3D, nes to tikėsis CNN\n",
    "X_stem_reshaped = X_stem.reshape((X_stem.shape[0], X_stem.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_stem_train, X_stem_test, y_stem_train, y_stem_test = train_test_split(X_stem_reshaped, y_stem, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, Input\n",
    "\n",
    "# Apsirašomas CNN modelis\n",
    "model_stem_tfidf_cnn = Sequential([\n",
    "    Input(shape=(X_stem_train.shape[1], 1)), \n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_stem_train.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Sukompiliuojamas modelis\n",
    "model_stem_tfidf_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Apsirasomas ankstyvas mokymosi sustabdymas.\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Galima naudot ir 'val_accuracy'\n",
    "    patience=3,  # Kai tiek epochų modelis nebetobuleja - bus sustabdytas mokymasis\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 23ms/step - accuracy: 0.3172 - loss: 2.4076 - val_accuracy: 0.4188 - val_loss: 1.9989\n",
      "Epoch 2/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4115 - loss: 2.0535 - val_accuracy: 0.4299 - val_loss: 1.9221\n",
      "Epoch 3/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4271 - loss: 1.9846 - val_accuracy: 0.4330 - val_loss: 1.8925\n",
      "Epoch 4/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4221 - loss: 1.9626 - val_accuracy: 0.4347 - val_loss: 1.8877\n",
      "Epoch 5/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4227 - loss: 1.9610 - val_accuracy: 0.4371 - val_loss: 1.8771\n",
      "Epoch 6/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4311 - loss: 1.9286 - val_accuracy: 0.4408 - val_loss: 1.8712\n",
      "Epoch 7/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4330 - loss: 1.9189 - val_accuracy: 0.4408 - val_loss: 1.8588\n",
      "Epoch 8/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4287 - loss: 1.9223 - val_accuracy: 0.4411 - val_loss: 1.8567\n",
      "Epoch 9/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4394 - loss: 1.9018 - val_accuracy: 0.4401 - val_loss: 1.8548\n",
      "Epoch 10/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4322 - loss: 1.8991 - val_accuracy: 0.4392 - val_loss: 1.8549\n"
     ]
    }
   ],
   "source": [
    "# Modelis mokosi apie 4 min\n",
    "history_stem_tfidf_cnn = model_stem_tfidf_cnn.fit(X_stem_train, y_stem_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taip pat issaugomas modelis, kad kiekviena kart jo nereiketu mokyti:\n",
    "model_stem_tfidf_cnn.save('models/stem_tfidf_cnn.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model_stem_tfidf_cnn = load_model('models/stem_tfidf_cnn.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n",
      "Stemingas + TF-IDF + CNN\n",
      "F1 Score: 0.339\n",
      "Tikslumas: 0.439\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = model_stem_tfidf_cnn.predict(X_stem_test)\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "y_true_classes = y_stem_test.argmax(axis=1) \n",
    "\n",
    "f1_stem_tfidf_cnn = round(f1_score(y_true_classes, y_pred_classes, average='weighted'), 3)\n",
    "accuracy_stem_tfidf_cnn = round(accuracy_score(y_true_classes, y_pred_classes), 3)\n",
    "\n",
    "\n",
    "print(\"Stemingas + TF-IDF + CNN\")\n",
    "print(f\"F1 Score: {f1_stem_tfidf_cnn}\")\n",
    "print(f\"Tikslumas: {accuracy_stem_tfidf_cnn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gautos stemingo + TF-IDF + cnn metrikos:\n",
    "Tikslumas: 0.439\n",
    "F1: 0.339\n",
    "\n",
    "Buvo Lemavimo + TF-IDF + CNN atveju:\n",
    "Tikslumas: 0.443\n",
    "F1: 0.354"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tad skirtumas ar lemavimas ar stemingas buvo naudotas CNN atveju nesudarė didelių skirtumų, kad limituoti naudojamus variantus, analizė bus tesiama su stemingu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC (Support vector classifier) modelis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is naujo apsirasau, nes SVC reikalauja kitokio input nei CNN atveju.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cleaned_data/stemming.csv\")\n",
    "X = tf_idf(stemming_df['DESCRIPTION'])\n",
    "y = stemming_df['GENRE']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_stem_tfidf_svc = SVC()\n",
    "history_stem_tfidf_svc = model_stem_tfidf_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/model_stem_tfidf_svc.joblib']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# issaugojimas modelio:\n",
    "# svc neturi atributo save, tai reikia kitaip:\n",
    "\n",
    "from joblib import dump\n",
    "dump(model_stem_tfidf_svc, 'models/model_stem_tfidf_svc.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "model_stem_tfidf_svc = load('models/model_stem_tfidf_svc.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' drama ', ' drama ', ' documentary ', ..., ' drama ',\n",
       "       ' documentary ', ' documentary '], dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_stem_tfidf_svc.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tikslumas: 0.471, F1: 0.402\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f1_stem_tfidf_svc = round(f1_score(y_test, y_pred, average='weighted'), 3)\n",
    "accuracy_stem_tfidf_svc = round(accuracy_score(y_test, y_pred), 3)\n",
    "\n",
    "print(f\"Tikslumas: {accuracy_stem_tfidf_svc}, F1: {f1_stem_tfidf_svc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paprasto SVC atveju gavome geresni tikslumas ir F1 nei baziniu CNN. (0.471 > 0.443, 0.402 > 0.354)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM modelis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Is naujo apsirasau, nes LSTM reikalauja kitokio input jei CNN ir SVC atveju.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cleaned_data/stemming.csv\")\n",
    "X = tf_idf(stemming_df['DESCRIPTION'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM modeliui reikalinga ivestis \n",
    "X = X.reshape((X.shape[0], 1, X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoded = onehot_encoder.fit_transform(stemming_df[\"GENRE\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54086, 1, 200)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54086, 27)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.layers import LSTM, Dense, SpatialDropout1D\n",
    "\n",
    "model_stem_tfidf_lstm = Sequential()\n",
    "model_stem_tfidf_lstm.add(SpatialDropout1D(0.2))\n",
    "model_stem_tfidf_lstm.add(LSTM(100, dropout=0.1, recurrent_dropout=0.2))\n",
    "model_stem_tfidf_lstm.add(Dense(27, activation='softmax'))\n",
    "model_stem_tfidf_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.3660 - loss: 2.3756 - val_accuracy: 0.4460 - val_loss: 1.8896\n",
      "Epoch 2/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4304 - loss: 1.9382 - val_accuracy: 0.4640 - val_loss: 1.8119\n",
      "Epoch 3/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4390 - loss: 1.8870 - val_accuracy: 0.4627 - val_loss: 1.7776\n",
      "Epoch 4/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4371 - loss: 1.8444 - val_accuracy: 0.4658 - val_loss: 1.7672\n",
      "Epoch 5/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4440 - loss: 1.8295 - val_accuracy: 0.4698 - val_loss: 1.7568\n",
      "Epoch 6/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4425 - loss: 1.8302 - val_accuracy: 0.4684 - val_loss: 1.7527\n",
      "Epoch 7/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4444 - loss: 1.8162 - val_accuracy: 0.4688 - val_loss: 1.7483\n",
      "Epoch 8/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4454 - loss: 1.8097 - val_accuracy: 0.4704 - val_loss: 1.7437\n",
      "Epoch 9/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.4428 - loss: 1.8088 - val_accuracy: 0.4729 - val_loss: 1.7425\n",
      "Epoch 10/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4477 - loss: 1.7873 - val_accuracy: 0.4699 - val_loss: 1.7416\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callback = [EarlyStopping(monitor='val_loss', min_delta=0, patience=2, mode='auto')]\n",
    "history_stem_tfidf_lstm = model_stem_tfidf_lstm.fit(X_train, y_train, epochs=10, batch_size=32,validation_data=(X_test,y_test), callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.17858176, 0.00246581, 0.03932749, ..., 0.0596112 , 0.00647582,\n",
       "        0.01319527],\n",
       "       [0.00949327, 0.00295327, 0.01420088, ..., 0.01966343, 0.00065109,\n",
       "        0.0016308 ],\n",
       "       [0.00972187, 0.00437902, 0.00543028, ..., 0.02277772, 0.0007758 ,\n",
       "        0.00429612],\n",
       "       ...,\n",
       "       [0.00866502, 0.00712521, 0.01910072, ..., 0.00987565, 0.00668057,\n",
       "        0.00145104],\n",
       "       [0.07787254, 0.00074805, 0.02200148, ..., 0.01088151, 0.00235594,\n",
       "        0.01125201],\n",
       "       [0.00697261, 0.00420403, 0.00578174, ..., 0.0062598 , 0.00038751,\n",
       "        0.00057033]], dtype=float32)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_stem_tfidf_lstm.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "y_true_classes = y_test.argmax(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tikslumas: 0.47, F1: 0.397\n"
     ]
    }
   ],
   "source": [
    "f1_stem_tfidf_lstm = round(f1_score(y_true_classes, y_pred_classes, average='weighted'), 3)\n",
    "accuracy_stem_tfidf_lstm = round(accuracy_score(y_true_classes, y_pred_classes), 3)\n",
    "\n",
    "print(f\"Tikslumas: {accuracy_stem_tfidf_lstm}, F1: {f1_stem_tfidf_lstm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM suveike labai panasiai kaip SVC siuo atveju."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DABAR tie patys trys modeliai CNN, LSTM, SVC bandomi su stemingu + word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cleaned_data/stemming.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def string_to_lists(row):\n",
    "    try:\n",
    "        row = ast.literal_eval(row)\n",
    "    except:\n",
    "        pass\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = df['DESCRIPTION'].apply(string_to_lists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(descriptions, vector_size=200, window=10, min_count=3, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = word2vec_model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'life': 0,\n",
       " 'one': 1,\n",
       " 'film': 2,\n",
       " 'live': 3,\n",
       " 'stori': 4,\n",
       " 'find': 5,\n",
       " 'world': 6,\n",
       " 'year': 7,\n",
       " 'new': 8,\n",
       " 'get': 9,\n",
       " 'love': 10,\n",
       " 'take': 11,\n",
       " 'famili': 12,\n",
       " 'young': 13,\n",
       " 'time': 14,\n",
       " 'two': 15,\n",
       " 'make': 16,\n",
       " 'friend': 17,\n",
       " 'man': 18,\n",
       " 'work': 19,\n",
       " 'peopl': 20,\n",
       " 'day': 21,\n",
       " 'becom': 22,\n",
       " 'come': 23,\n",
       " 'father': 24,\n",
       " 'show': 25,\n",
       " 'way': 26,\n",
       " 'girl': 27,\n",
       " 'first': 28,\n",
       " 'home': 29,\n",
       " 'tri': 30,\n",
       " 'also': 31,\n",
       " 'meet': 32,\n",
       " 'go': 33,\n",
       " 'follow': 34,\n",
       " 'back': 35,\n",
       " 'help': 36,\n",
       " 'documentari': 37,\n",
       " 'old': 38,\n",
       " 'mother': 39,\n",
       " 'woman': 40,\n",
       " 'want': 41,\n",
       " 'son': 42,\n",
       " 'look': 43,\n",
       " 'citi': 44,\n",
       " 'like': 45,\n",
       " 'turn': 46,\n",
       " 'decid': 47,\n",
       " 'end': 48,\n",
       " 'wife': 49,\n",
       " 'set': 50,\n",
       " 'three': 51,\n",
       " 'school': 52,\n",
       " 'see': 53,\n",
       " 'start': 54,\n",
       " 'begin': 55,\n",
       " 'town': 56,\n",
       " 'place': 57,\n",
       " 'use': 58,\n",
       " 'hous': 59,\n",
       " 'tell': 60,\n",
       " 'chang': 61,\n",
       " 'person': 62,\n",
       " 'music': 63,\n",
       " 'daughter': 64,\n",
       " 'even': 65,\n",
       " 'discov': 66,\n",
       " 'leav': 67,\n",
       " 'mani': 68,\n",
       " 'kill': 69,\n",
       " 'return': 70,\n",
       " 'forc': 71,\n",
       " 'play': 72,\n",
       " 'journey': 73,\n",
       " 'death': 74,\n",
       " 'women': 75,\n",
       " 'seri': 76,\n",
       " 'dream': 77,\n",
       " 'war': 78,\n",
       " 'togeth': 79,\n",
       " 'marri': 80,\n",
       " 'know': 81,\n",
       " 'face': 82,\n",
       " 'fall': 83,\n",
       " 'us': 84,\n",
       " 'name': 85,\n",
       " 'group': 86,\n",
       " 'brother': 87,\n",
       " 'night': 88,\n",
       " 'call': 89,\n",
       " 'boy': 90,\n",
       " 'includ': 91,\n",
       " 'around': 92,\n",
       " 'give': 93,\n",
       " 'must': 94,\n",
       " 'children': 95,\n",
       " 'differ': 96,\n",
       " 'learn': 97,\n",
       " 'well': 98,\n",
       " 'lead': 99,\n",
       " 'american': 100,\n",
       " 'movi': 101,\n",
       " 'thing': 102,\n",
       " 'countri': 103,\n",
       " 'men': 104,\n",
       " 'run': 105,\n",
       " 'murder': 106,\n",
       " 'relationship': 107,\n",
       " 'final': 108,\n",
       " 'struggl': 109,\n",
       " 'howev': 110,\n",
       " 'best': 111,\n",
       " 'money': 112,\n",
       " 'featur': 113,\n",
       " 'experi': 114,\n",
       " 'past': 115,\n",
       " 'bring': 116,\n",
       " 'power': 117,\n",
       " 'move': 118,\n",
       " 'beauti': 119,\n",
       " 'goe': 120,\n",
       " 'local': 121,\n",
       " 'fight': 122,\n",
       " 'human': 123,\n",
       " 'would': 124,\n",
       " 'artist': 125,\n",
       " 'commun': 126,\n",
       " 'job': 127,\n",
       " 'husband': 128,\n",
       " 'last': 129,\n",
       " 'secret': 130,\n",
       " 'never': 131,\n",
       " 'explor': 132,\n",
       " 'histori': 133,\n",
       " 'soon': 134,\n",
       " 'plan': 135,\n",
       " 'away': 136,\n",
       " 'small': 137,\n",
       " 'charact': 138,\n",
       " 'event': 139,\n",
       " 'littl': 140,\n",
       " 'part': 141,\n",
       " 'die': 142,\n",
       " 'anoth': 143,\n",
       " 'hope': 144,\n",
       " 'long': 145,\n",
       " 'real': 146,\n",
       " 'student': 147,\n",
       " 'interview': 148,\n",
       " 'team': 149,\n",
       " 'perform': 150,\n",
       " 'creat': 151,\n",
       " 'art': 152,\n",
       " 'mysteri': 153,\n",
       " 'villag': 154,\n",
       " 'travel': 155,\n",
       " 'state': 156,\n",
       " 'sister': 157,\n",
       " 'star': 158,\n",
       " 'save': 159,\n",
       " 'parent': 160,\n",
       " 'everi': 161,\n",
       " 'feel': 162,\n",
       " 'made': 163,\n",
       " 'order': 164,\n",
       " 'game': 165,\n",
       " 'polic': 166,\n",
       " 'happen': 167,\n",
       " 'present': 168,\n",
       " 'good': 169,\n",
       " 'arriv': 170,\n",
       " 'need': 171,\n",
       " 'question': 172,\n",
       " 'behind': 173,\n",
       " 'still': 174,\n",
       " 'along': 175,\n",
       " 'success': 176,\n",
       " 'busi': 177,\n",
       " 'left': 178,\n",
       " 'later': 179,\n",
       " 'cultur': 180,\n",
       " 'high': 181,\n",
       " 'attempt': 182,\n",
       " 'keep': 183,\n",
       " 'great': 184,\n",
       " 'true': 185,\n",
       " 'much': 186,\n",
       " 'search': 187,\n",
       " 'escap': 188,\n",
       " 'reveal': 189,\n",
       " 'seem': 190,\n",
       " 'street': 191,\n",
       " 'put': 192,\n",
       " 'age': 193,\n",
       " 'believ': 194,\n",
       " 'director': 195,\n",
       " 'child': 196,\n",
       " 'big': 197,\n",
       " 'four': 198,\n",
       " 'found': 199,\n",
       " 'hand': 200,\n",
       " 'without': 201,\n",
       " 'everyth': 202,\n",
       " 'base': 203,\n",
       " 'de': 204,\n",
       " 'he': 205,\n",
       " 'may': 206,\n",
       " 'visit': 207,\n",
       " 'problem': 208,\n",
       " 'coupl': 209,\n",
       " 'head': 210,\n",
       " 'right': 211,\n",
       " 'lost': 212,\n",
       " 'ever': 213,\n",
       " 'could': 214,\n",
       " 'gener': 215,\n",
       " 'realiz': 216,\n",
       " 'nation': 217,\n",
       " 'surviv': 218,\n",
       " 'former': 219,\n",
       " 'sinc': 220,\n",
       " 'realiti': 221,\n",
       " 'polit': 222,\n",
       " 'america': 223,\n",
       " 'scene': 224,\n",
       " 'parti': 225,\n",
       " 'john': 226,\n",
       " 'ask': 227,\n",
       " 'heart': 228,\n",
       " 'whose': 229,\n",
       " 'join': 230,\n",
       " 'member': 231,\n",
       " 'land': 232,\n",
       " 'offer': 233,\n",
       " 'produc': 234,\n",
       " 'share': 235,\n",
       " 'la': 236,\n",
       " 'next': 237,\n",
       " 'futur': 238,\n",
       " 'challeng': 239,\n",
       " 'offic': 240,\n",
       " 'mean': 241,\n",
       " 'build': 242,\n",
       " 'car': 243,\n",
       " 'dark': 244,\n",
       " 'break': 245,\n",
       " 'york': 246,\n",
       " 'investig': 247,\n",
       " 'video': 248,\n",
       " 'think': 249,\n",
       " 'special': 250,\n",
       " 'compani': 251,\n",
       " 'prison': 252,\n",
       " 'drug': 253,\n",
       " 'celebr': 254,\n",
       " 'win': 255,\n",
       " 'known': 256,\n",
       " 'deal': 257,\n",
       " 'open': 258,\n",
       " 'continu': 259,\n",
       " 'sever': 260,\n",
       " 'social': 261,\n",
       " 'marriag': 262,\n",
       " 'manag': 263,\n",
       " 'natur': 264,\n",
       " 'origin': 265,\n",
       " 'develop': 266,\n",
       " 'alway': 267,\n",
       " 'someth': 268,\n",
       " 'other': 269,\n",
       " 'stay': 270,\n",
       " 'bodi': 271,\n",
       " 'inspir': 272,\n",
       " 'societi': 273,\n",
       " 'career': 274,\n",
       " 'filmmak': 275,\n",
       " 'gang': 276,\n",
       " 'stop': 277,\n",
       " 'adventur': 278,\n",
       " 'short': 279,\n",
       " 'across': 280,\n",
       " 'comedi': 281,\n",
       " 'side': 282,\n",
       " 'dead': 283,\n",
       " 'form': 284,\n",
       " 'kid': 285,\n",
       " 'appear': 286,\n",
       " 'care': 287,\n",
       " 'close': 288,\n",
       " 'train': 289,\n",
       " 'battl': 290,\n",
       " 'black': 291,\n",
       " 'involv': 292,\n",
       " 'lose': 293,\n",
       " 'truth': 294,\n",
       " 'talk': 295,\n",
       " 'grow': 296,\n",
       " 'fear': 297,\n",
       " 'memori': 298,\n",
       " 'yet': 299,\n",
       " 'act': 300,\n",
       " 'watch': 301,\n",
       " 'uniqu': 302,\n",
       " 'island': 303,\n",
       " 'tv': 304,\n",
       " 'five': 305,\n",
       " 'band': 306,\n",
       " 'shot': 307,\n",
       " 'complet': 308,\n",
       " 'situat': 309,\n",
       " 'seek': 310,\n",
       " 'danc': 311,\n",
       " 'unit': 312,\n",
       " 'intern': 313,\n",
       " 'captur': 314,\n",
       " 'apart': 315,\n",
       " 'today': 316,\n",
       " 'caus': 317,\n",
       " 'realli': 318,\n",
       " 'crime': 319,\n",
       " 'upon': 320,\n",
       " 'anim': 321,\n",
       " 'record': 322,\n",
       " 'eye': 323,\n",
       " 'second': 324,\n",
       " 'dr': 325,\n",
       " 'actor': 326,\n",
       " 'govern': 327,\n",
       " 'case': 328,\n",
       " 'modern': 329,\n",
       " 'danger': 330,\n",
       " 'girlfriend': 331,\n",
       " 'result': 332,\n",
       " 'colleg': 333,\n",
       " 'troubl': 334,\n",
       " 'mr': 335,\n",
       " 'teenag': 336,\n",
       " 'happi': 337,\n",
       " 'doctor': 338,\n",
       " 'sex': 339,\n",
       " 'emot': 340,\n",
       " 'action': 341,\n",
       " 'pass': 342,\n",
       " 'point': 343,\n",
       " 'univers': 344,\n",
       " 'race': 345,\n",
       " 'control': 346,\n",
       " 'mind': 347,\n",
       " 'public': 348,\n",
       " 'footag': 349,\n",
       " 'remain': 350,\n",
       " 'provid': 351,\n",
       " 'spend': 352,\n",
       " 'book': 353,\n",
       " 'host': 354,\n",
       " 'trip': 355,\n",
       " 'road': 356,\n",
       " 'encount': 357,\n",
       " 'project': 358,\n",
       " 'accept': 359,\n",
       " 'miss': 360,\n",
       " 'chanc': 361,\n",
       " 'centuri': 362,\n",
       " 'episod': 363,\n",
       " 'support': 364,\n",
       " 'import': 365,\n",
       " 'rich': 366,\n",
       " 'moment': 367,\n",
       " 'say': 368,\n",
       " 'insid': 369,\n",
       " 'within': 370,\n",
       " 'qv': 371,\n",
       " 'program': 372,\n",
       " 'south': 373,\n",
       " 'interest': 374,\n",
       " 'everyon': 375,\n",
       " 'product': 376,\n",
       " 'tradit': 377,\n",
       " 'room': 378,\n",
       " 'teacher': 379,\n",
       " 'understand': 380,\n",
       " 'owner': 381,\n",
       " 'answer': 382,\n",
       " 'possibl': 383,\n",
       " 'stage': 384,\n",
       " 'exist': 385,\n",
       " 'drama': 386,\n",
       " 'seen': 387,\n",
       " 'industri': 388,\n",
       " 'ultim': 389,\n",
       " 'jack': 390,\n",
       " 'camera': 391,\n",
       " 'role': 392,\n",
       " 'guy': 393,\n",
       " 'angel': 394,\n",
       " 'wit': 395,\n",
       " 'victim': 396,\n",
       " 'famou': 397,\n",
       " 'noth': 398,\n",
       " 'leader': 399,\n",
       " 'sexual': 400,\n",
       " 'better': 401,\n",
       " 'tale': 402,\n",
       " 'david': 403,\n",
       " 'issu': 404,\n",
       " 'desper': 405,\n",
       " 'taken': 406,\n",
       " 'week': 407,\n",
       " 'late': 408,\n",
       " 'passion': 409,\n",
       " 'full': 410,\n",
       " 'attack': 411,\n",
       " 'killer': 412,\n",
       " 'water': 413,\n",
       " 'stand': 414,\n",
       " 'light': 415,\n",
       " 'confront': 416,\n",
       " 'audienc': 417,\n",
       " 'month': 418,\n",
       " 'idea': 419,\n",
       " 'king': 420,\n",
       " 'televis': 421,\n",
       " 'releas': 422,\n",
       " 'crew': 423,\n",
       " 'locat': 424,\n",
       " 'walk': 425,\n",
       " 'rais': 426,\n",
       " 'white': 427,\n",
       " 'hard': 428,\n",
       " 'evil': 429,\n",
       " 'determin': 430,\n",
       " 'abl': 431,\n",
       " 'strang': 432,\n",
       " 'suffer': 433,\n",
       " 'eventu': 434,\n",
       " 'view': 435,\n",
       " 'alon': 436,\n",
       " 'line': 437,\n",
       " 'hit': 438,\n",
       " 'friendship': 439,\n",
       " 'lover': 440,\n",
       " 'ident': 441,\n",
       " 'hour': 442,\n",
       " 'hospit': 443,\n",
       " 'commit': 444,\n",
       " 'viewer': 445,\n",
       " 'rock': 446,\n",
       " 'process': 447,\n",
       " 'earli': 448,\n",
       " 'thought': 449,\n",
       " 'enter': 450,\n",
       " 'far': 451,\n",
       " 'earth': 452,\n",
       " 'word': 453,\n",
       " 'spirit': 454,\n",
       " 'among': 455,\n",
       " 'million': 456,\n",
       " 'despit': 457,\n",
       " 'mountain': 458,\n",
       " 'top': 459,\n",
       " 'babi': 460,\n",
       " 'direct': 461,\n",
       " 'childhood': 462,\n",
       " 'cours': 463,\n",
       " 'cast': 464,\n",
       " 'connect': 465,\n",
       " 'organ': 466,\n",
       " 'inform': 467,\n",
       " 'author': 468,\n",
       " 'told': 469,\n",
       " 'drive': 470,\n",
       " 'fact': 471,\n",
       " 'ago': 472,\n",
       " 'hold': 473,\n",
       " 'let': 474,\n",
       " 'free': 475,\n",
       " 'receiv': 476,\n",
       " 'popular': 477,\n",
       " 'recent': 478,\n",
       " 'mission': 479,\n",
       " 'collect': 480,\n",
       " 'imag': 481,\n",
       " 'class': 482,\n",
       " 'pay': 483,\n",
       " 'design': 484,\n",
       " 'photograph': 485,\n",
       " 'prepar': 486,\n",
       " 'though': 487,\n",
       " 'often': 488,\n",
       " 'writer': 489,\n",
       " 'reach': 490,\n",
       " 'club': 491,\n",
       " 'educ': 492,\n",
       " 'armi': 493,\n",
       " 'actual': 494,\n",
       " 'law': 495,\n",
       " 'protect': 496,\n",
       " 'news': 497,\n",
       " 'hollywood': 498,\n",
       " 'food': 499,\n",
       " 'deep': 500,\n",
       " 'michael': 501,\n",
       " 'accid': 502,\n",
       " 'entir': 503,\n",
       " 'summer': 504,\n",
       " 'center': 505,\n",
       " 'entertain': 506,\n",
       " 'song': 507,\n",
       " 'becam': 508,\n",
       " 'hero': 509,\n",
       " 'cant': 510,\n",
       " 'test': 511,\n",
       " 'ladi': 512,\n",
       " 'mari': 513,\n",
       " 'prove': 514,\n",
       " 'bad': 515,\n",
       " 'due': 516,\n",
       " 'outsid': 517,\n",
       " 'sell': 518,\n",
       " 'camp': 519,\n",
       " 'mark': 520,\n",
       " 'report': 521,\n",
       " 'variou': 522,\n",
       " 'six': 523,\n",
       " 'meanwhil': 524,\n",
       " 'femal': 525,\n",
       " 'toward': 526,\n",
       " 'respons': 527,\n",
       " 'convinc': 528,\n",
       " 'date': 529,\n",
       " 'ride': 530,\n",
       " 'tom': 531,\n",
       " 'rise': 532,\n",
       " 'talent': 533,\n",
       " 'boyfriend': 534,\n",
       " 'singl': 535,\n",
       " 'born': 536,\n",
       " 'enough': 537,\n",
       " 'disappear': 538,\n",
       " 'affair': 539,\n",
       " 'threaten': 540,\n",
       " 'larg': 541,\n",
       " 'suicid': 542,\n",
       " 'major': 543,\n",
       " 'attract': 544,\n",
       " 'agent': 545,\n",
       " 'area': 546,\n",
       " 'lo': 547,\n",
       " 'write': 548,\n",
       " 'individu': 549,\n",
       " 'allow': 550,\n",
       " 'refus': 551,\n",
       " 'studi': 552,\n",
       " 'conflict': 553,\n",
       " 'figur': 554,\n",
       " 'player': 555,\n",
       " 'send': 556,\n",
       " 'london': 557,\n",
       " 'paul': 558,\n",
       " 'examin': 559,\n",
       " 'fail': 560,\n",
       " 'promis': 561,\n",
       " 'document': 562,\n",
       " 'space': 563,\n",
       " 'fire': 564,\n",
       " 'god': 565,\n",
       " 'effect': 566,\n",
       " 'pictur': 567,\n",
       " 'given': 568,\n",
       " 'carri': 569,\n",
       " 'detect': 570,\n",
       " 'abandon': 571,\n",
       " 'effort': 572,\n",
       " 'rescu': 573,\n",
       " 'youth': 574,\n",
       " 'invit': 575,\n",
       " 'profession': 576,\n",
       " 'voic': 577,\n",
       " 'wrong': 578,\n",
       " 'oper': 579,\n",
       " 'wait': 580,\n",
       " 'path': 581,\n",
       " 'lie': 582,\n",
       " 'middl': 583,\n",
       " 'suddenli': 584,\n",
       " 'peac': 585,\n",
       " 'soldier': 586,\n",
       " 'hear': 587,\n",
       " 'system': 588,\n",
       " 'sent': 589,\n",
       " 'militari': 590,\n",
       " 'soul': 591,\n",
       " 'desir': 592,\n",
       " 'air': 593,\n",
       " 'north': 594,\n",
       " 'decis': 595,\n",
       " 'fate': 596,\n",
       " 'almost': 597,\n",
       " 'brought': 598,\n",
       " 'west': 599,\n",
       " 'crimin': 600,\n",
       " 'worker': 601,\n",
       " 'faith': 602,\n",
       " 'boss': 603,\n",
       " 'cross': 604,\n",
       " 'wild': 605,\n",
       " 'destroy': 606,\n",
       " 'tour': 607,\n",
       " 'matter': 608,\n",
       " 'piec': 609,\n",
       " 'german': 610,\n",
       " 'mental': 611,\n",
       " 'partner': 612,\n",
       " 'dog': 613,\n",
       " 'transform': 614,\n",
       " 'hire': 615,\n",
       " 'number': 616,\n",
       " 'expect': 617,\n",
       " 'step': 618,\n",
       " 'sens': 619,\n",
       " 'arrest': 620,\n",
       " 'kind': 621,\n",
       " 'plot': 622,\n",
       " 'shoot': 623,\n",
       " 'engag': 624,\n",
       " 'indian': 625,\n",
       " 'farm': 626,\n",
       " 'lot': 627,\n",
       " 'jame': 628,\n",
       " 'choic': 629,\n",
       " 'imagin': 630,\n",
       " 'reveng': 631,\n",
       " 'someon': 632,\n",
       " 'bill': 633,\n",
       " 'haunt': 634,\n",
       " 'instead': 635,\n",
       " 'rule': 636,\n",
       " 'magic': 637,\n",
       " 'serv': 638,\n",
       " 'le': 639,\n",
       " 'surpris': 640,\n",
       " 'servic': 641,\n",
       " 'anyth': 642,\n",
       " 'fan': 643,\n",
       " 'poor': 644,\n",
       " 'histor': 645,\n",
       " 'sound': 646,\n",
       " 'got': 647,\n",
       " 'pain': 648,\n",
       " 'hide': 649,\n",
       " 'wealthi': 650,\n",
       " 'research': 651,\n",
       " 'reason': 652,\n",
       " 'master': 653,\n",
       " 'movement': 654,\n",
       " 'main': 655,\n",
       " 'front': 656,\n",
       " 'hunt': 657,\n",
       " 'discuss': 658,\n",
       " 'although': 659,\n",
       " 'media': 660,\n",
       " 'wed': 661,\n",
       " 'throughout': 662,\n",
       " 'abus': 663,\n",
       " 'quickli': 664,\n",
       " 'festiv': 665,\n",
       " 'resid': 666,\n",
       " 'current': 667,\n",
       " 'activ': 668,\n",
       " 'innoc': 669,\n",
       " 'kidnap': 670,\n",
       " 'claim': 671,\n",
       " 'quit': 672,\n",
       " 'musician': 673,\n",
       " 'track': 674,\n",
       " 'affect': 675,\n",
       " 'led': 676,\n",
       " 'georg': 677,\n",
       " 'classic': 678,\n",
       " 'perfect': 679,\n",
       " 'forev': 680,\n",
       " 'style': 681,\n",
       " 'park': 682,\n",
       " 'cover': 683,\n",
       " 'model': 684,\n",
       " 'daili': 685,\n",
       " 'surround': 686,\n",
       " 'teach': 687,\n",
       " 'sea': 688,\n",
       " 'respect': 689,\n",
       " 'door': 690,\n",
       " 'came': 691,\n",
       " 'took': 692,\n",
       " 'violenc': 693,\n",
       " 'award': 694,\n",
       " 'horror': 695,\n",
       " 'opportun': 696,\n",
       " 'might': 697,\n",
       " 'civil': 698,\n",
       " 'assist': 699,\n",
       " 'ancient': 700,\n",
       " 'joe': 701,\n",
       " 'peter': 702,\n",
       " 'hotel': 703,\n",
       " 'presid': 704,\n",
       " 'focus': 705,\n",
       " 'physic': 706,\n",
       " 'ill': 707,\n",
       " 'freedom': 708,\n",
       " 'guest': 709,\n",
       " 'whole': 710,\n",
       " 'thousand': 711,\n",
       " 'beyond': 712,\n",
       " 'british': 713,\n",
       " 'sport': 714,\n",
       " 'independ': 715,\n",
       " 'quest': 716,\n",
       " 'posit': 717,\n",
       " 'obsess': 718,\n",
       " 'robert': 719,\n",
       " 'intim': 720,\n",
       " 'medic': 721,\n",
       " 'privat': 722,\n",
       " 'condit': 723,\n",
       " 'health': 724,\n",
       " 'contest': 725,\n",
       " 'portrait': 726,\n",
       " 'shock': 727,\n",
       " 'practic': 728,\n",
       " 'wonder': 729,\n",
       " 'subject': 730,\n",
       " 'neighbor': 731,\n",
       " 'red': 732,\n",
       " 'bank': 733,\n",
       " 'sometim': 734,\n",
       " 'speak': 735,\n",
       " 'fun': 736,\n",
       " 'india': 737,\n",
       " '2': 738,\n",
       " 'approach': 739,\n",
       " 'river': 740,\n",
       " 'steal': 741,\n",
       " 'relat': 742,\n",
       " 'desert': 743,\n",
       " 'jim': 744,\n",
       " 'church': 745,\n",
       " 'aid': 746,\n",
       " 'caught': 747,\n",
       " 'hidden': 748,\n",
       " 'hors': 749,\n",
       " 'adult': 750,\n",
       " 'rest': 751,\n",
       " 'near': 752,\n",
       " 'uncl': 753,\n",
       " 'fill': 754,\n",
       " 'buy': 755,\n",
       " 'tragic': 756,\n",
       " 'fortun': 757,\n",
       " 'went': 758,\n",
       " 'birth': 759,\n",
       " 'greatest': 760,\n",
       " 'stranger': 761,\n",
       " 'whether': 762,\n",
       " 'california': 763,\n",
       " 'season': 764,\n",
       " 'blood': 765,\n",
       " 'ghost': 766,\n",
       " '3': 767,\n",
       " 'agre': 768,\n",
       " 'titl': 769,\n",
       " 'arm': 770,\n",
       " 'paint': 771,\n",
       " 'separ': 772,\n",
       " 'younger': 773,\n",
       " 'scientist': 774,\n",
       " 'suspect': 775,\n",
       " 'decad': 776,\n",
       " 'older': 777,\n",
       " 'europ': 778,\n",
       " 'actress': 779,\n",
       " 'professor': 780,\n",
       " 'drink': 781,\n",
       " 'gold': 782,\n",
       " 'extrem': 783,\n",
       " 'complic': 784,\n",
       " 'loss': 785,\n",
       " 'station': 786,\n",
       " 'expert': 787,\n",
       " 'william': 788,\n",
       " 'secur': 789,\n",
       " 'emerg': 790,\n",
       " 'consid': 791,\n",
       " 'cannot': 792,\n",
       " 'introduc': 793,\n",
       " 'gay': 794,\n",
       " 'shop': 795,\n",
       " 'french': 796,\n",
       " 'addict': 797,\n",
       " 'frank': 798,\n",
       " 'enjoy': 799,\n",
       " 'choos': 800,\n",
       " 'singer': 801,\n",
       " 'reflect': 802,\n",
       " 'creativ': 803,\n",
       " 'influenc': 804,\n",
       " 'fashion': 805,\n",
       " 'dramat': 806,\n",
       " 'competit': 807,\n",
       " 'pick': 808,\n",
       " 'attent': 809,\n",
       " 'bar': 810,\n",
       " 'execut': 811,\n",
       " 'legend': 812,\n",
       " 'ten': 813,\n",
       " 'studio': 814,\n",
       " 'trap': 815,\n",
       " 'journalist': 816,\n",
       " 'unknown': 817,\n",
       " 'minut': 818,\n",
       " 'alreadi': 819,\n",
       " 'expos': 820,\n",
       " 'brutal': 821,\n",
       " 'adopt': 822,\n",
       " 'strong': 823,\n",
       " 'achiev': 824,\n",
       " 'combin': 825,\n",
       " 'aliv': 826,\n",
       " 'catch': 827,\n",
       " 'simpl': 828,\n",
       " 'east': 829,\n",
       " 'field': 830,\n",
       " 'difficult': 831,\n",
       " 'met': 832,\n",
       " 'charg': 833,\n",
       " 'immigr': 834,\n",
       " 'tragedi': 835,\n",
       " 'gain': 836,\n",
       " 'anyon': 837,\n",
       " 'san': 838,\n",
       " 'seven': 839,\n",
       " 'hell': 840,\n",
       " 'wish': 841,\n",
       " 'border': 842,\n",
       " 'period': 843,\n",
       " 'touch': 844,\n",
       " 'attend': 845,\n",
       " 'common': 846,\n",
       " 'ship': 847,\n",
       " 'rare': 848,\n",
       " 'unfortun': 849,\n",
       " 'enemi': 850,\n",
       " 'divorc': 851,\n",
       " 'fascin': 852,\n",
       " 'christma': 853,\n",
       " 'color': 854,\n",
       " 'sam': 855,\n",
       " 'pregnant': 856,\n",
       " 'bob': 857,\n",
       " 'letter': 858,\n",
       " 'demon': 859,\n",
       " 'pari': 860,\n",
       " 'guid': 861,\n",
       " 'sing': 862,\n",
       " 'crisi': 863,\n",
       " 'driver': 864,\n",
       " 'widow': 865,\n",
       " 'unexpect': 866,\n",
       " 'readi': 867,\n",
       " 'region': 868,\n",
       " 'court': 869,\n",
       " 'africa': 870,\n",
       " 'consequ': 871,\n",
       " 'comic': 872,\n",
       " 'global': 873,\n",
       " 'wall': 874,\n",
       " 'biggest': 875,\n",
       " 'violent': 876,\n",
       " 'excit': 877,\n",
       " 'uncov': 878,\n",
       " 'unabl': 879,\n",
       " 'china': 880,\n",
       " 'justic': 881,\n",
       " 'western': 882,\n",
       " 'cut': 883,\n",
       " 'rather': 884,\n",
       " 'critic': 885,\n",
       " 'perspect': 886,\n",
       " 'forest': 887,\n",
       " 'japanes': 888,\n",
       " 'mexico': 889,\n",
       " 'everyday': 890,\n",
       " 'richard': 891,\n",
       " 'financi': 892,\n",
       " 'bond': 893,\n",
       " 'especi': 894,\n",
       " 'overcom': 895,\n",
       " 'neighborhood': 896,\n",
       " 'sign': 897,\n",
       " 'morn': 898,\n",
       " 'read': 899,\n",
       " 'els': 900,\n",
       " 'express': 901,\n",
       " 'chronicl': 902,\n",
       " 'male': 903,\n",
       " 'lifestyl': 904,\n",
       " 'extraordinari': 905,\n",
       " 'chase': 906,\n",
       " 'patient': 907,\n",
       " 'mike': 908,\n",
       " 'wood': 909,\n",
       " 'retir': 910,\n",
       " 'dan': 911,\n",
       " 'dancer': 912,\n",
       " 'daniel': 913,\n",
       " 'complex': 914,\n",
       " 'fiction': 915,\n",
       " '20': 916,\n",
       " 'environ': 917,\n",
       " 'written': 918,\n",
       " 'un': 919,\n",
       " 'twist': 920,\n",
       " 'christian': 921,\n",
       " 'hot': 922,\n",
       " 'nativ': 923,\n",
       " 'fli': 924,\n",
       " 'pursu': 925,\n",
       " 'store': 926,\n",
       " 'who': 927,\n",
       " 'particip': 928,\n",
       " 'gun': 929,\n",
       " 'fame': 930,\n",
       " 'wake': 931,\n",
       " 'cinema': 932,\n",
       " 'impact': 933,\n",
       " 'half': 934,\n",
       " 'di': 935,\n",
       " 'chines': 936,\n",
       " 'alien': 937,\n",
       " 'valu': 938,\n",
       " 'farmer': 939,\n",
       " 'anna': 940,\n",
       " 'landscap': 941,\n",
       " 'unfold': 942,\n",
       " 'technolog': 943,\n",
       " 'insight': 944,\n",
       " 'mix': 945,\n",
       " 'initi': 946,\n",
       " '10': 947,\n",
       " 'planet': 948,\n",
       " 'african': 949,\n",
       " 'spiritu': 950,\n",
       " 'board': 951,\n",
       " 'risk': 952,\n",
       " 'captain': 953,\n",
       " '30': 954,\n",
       " 'blue': 955,\n",
       " 'repres': 956,\n",
       " 'fish': 957,\n",
       " 'lone': 958,\n",
       " 'econom': 959,\n",
       " 'contemporari': 960,\n",
       " 'box': 961,\n",
       " 'remot': 962,\n",
       " 'explain': 963,\n",
       " 'clear': 964,\n",
       " 'vision': 965,\n",
       " 'screen': 966,\n",
       " 'adam': 967,\n",
       " 'visual': 968,\n",
       " 'trial': 969,\n",
       " 'urban': 970,\n",
       " 'sheriff': 971,\n",
       " 'italian': 972,\n",
       " 'legendari': 973,\n",
       " 'ranch': 974,\n",
       " 'queen': 975,\n",
       " 'goal': 976,\n",
       " 'dad': 977,\n",
       " 'embark': 978,\n",
       " 'radio': 979,\n",
       " 'gone': 980,\n",
       " 'harri': 981,\n",
       " 'corrupt': 982,\n",
       " 'term': 983,\n",
       " 'beach': 984,\n",
       " 'account': 985,\n",
       " 'terror': 986,\n",
       " 'lee': 987,\n",
       " 'martin': 988,\n",
       " 'began': 989,\n",
       " 'fellow': 990,\n",
       " 'skill': 991,\n",
       " 'nightmar': 992,\n",
       " 'draw': 993,\n",
       " 'broken': 994,\n",
       " 'third': 995,\n",
       " 'prostitut': 996,\n",
       " 'franc': 997,\n",
       " 'mine': 998,\n",
       " 'survivor': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = []\n",
    "for tokens in descriptions:\n",
    "    word_embeddings = [word_vectors[word] for word in tokens if word in word_vectors]\n",
    "    if word_embeddings:\n",
    "        document_embedding = sum(word_embeddings) / len(word_embeddings)\n",
    "    else:\n",
    "        document_embedding = [0.0] * 200\n",
    "    document_embeddings.append(document_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_document_embeddings = scaler.fit_transform(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = scaled_document_embeddings\n",
    "y = onehot_encoder.fit_transform(stemming_df[\"GENRE\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming + Word2Vec + CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "\n",
    "# Apsirašomas CNN modelis \n",
    "model_stem_wordvec_cnn = Sequential([\n",
    "    Input(shape=(X_train.shape[1], 1)),  \n",
    "    Conv1D(filters=128, kernel_size=5, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(y_train.shape[1], activation='softmax')\n",
    "])\n",
    "\n",
    "# Sukompiliuojamas modelis\n",
    "model_stem_wordvec_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Apsirasomas ankstyvas mokymosi sustabdymas.\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',  # Galima naudot ir 'val_accuracy'\n",
    "    patience=3,  # Kai tiek epochų modelis nebetobuleja - bus sustabdytas mokymasis\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 23ms/step - accuracy: 0.3597 - loss: 2.2494 - val_accuracy: 0.4839 - val_loss: 1.8766\n",
      "Epoch 2/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4467 - loss: 1.9258 - val_accuracy: 0.4920 - val_loss: 1.7373\n",
      "Epoch 3/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4553 - loss: 1.8799 - val_accuracy: 0.4993 - val_loss: 1.7428\n",
      "Epoch 4/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4603 - loss: 1.8331 - val_accuracy: 0.5088 - val_loss: 1.6843\n",
      "Epoch 5/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4676 - loss: 1.8053 - val_accuracy: 0.5037 - val_loss: 1.6661\n",
      "Epoch 6/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4694 - loss: 1.7942 - val_accuracy: 0.5049 - val_loss: 1.6463\n",
      "Epoch 7/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4689 - loss: 1.7786 - val_accuracy: 0.5032 - val_loss: 1.6646\n",
      "Epoch 8/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4735 - loss: 1.7592 - val_accuracy: 0.5081 - val_loss: 1.6243\n",
      "Epoch 9/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4795 - loss: 1.7462 - val_accuracy: 0.5122 - val_loss: 1.6229\n",
      "Epoch 10/10\n",
      "\u001b[1m1082/1082\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 23ms/step - accuracy: 0.4729 - loss: 1.7504 - val_accuracy: 0.5132 - val_loss: 1.6088\n"
     ]
    }
   ],
   "source": [
    "# Modelis mokosi apie 4 min\n",
    "history_stem_wordvec_cnn = model_stem_wordvec_cnn.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issaugojimas\n",
    "model_stem_wordvec_cnn.save('models/model_stem_wordvec_cnn.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uzkrovimas issaugoto modelio\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "model_stem_wordvec_cnn = load_model('models/model_stem_wordvec_cnn.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[9.4410613e-02, 2.9639041e-03, 3.0168671e-02, ..., 7.8234002e-02,\n",
       "        3.5943172e-03, 1.7354399e-03],\n",
       "       [4.4781920e-03, 1.3830594e-03, 8.6351596e-03, ..., 6.4346604e-03,\n",
       "        1.4277286e-03, 3.7453757e-04],\n",
       "       [1.1521258e-03, 9.0423319e-03, 6.0910205e-03, ..., 4.3110582e-03,\n",
       "        6.3256957e-05, 2.2181788e-05],\n",
       "       ...,\n",
       "       [3.6405330e-03, 1.9913728e-03, 5.7395748e-03, ..., 2.7241418e-03,\n",
       "        1.3934788e-03, 2.4272301e-04],\n",
       "       [4.9695000e-02, 6.2846212e-04, 2.7148476e-02, ..., 2.1305813e-02,\n",
       "        1.4765844e-02, 5.1160357e-03],\n",
       "       [5.8731274e-04, 1.7601098e-05, 1.5449696e-03, ..., 4.6609747e-04,\n",
       "        2.6799407e-04, 2.3109119e-06]], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_stem_wordvec_cnn.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konvertuojam i daugiausia procentu duotas klases\n",
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "y_true_classes = y_test.argmax(axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tikslumas: 0.512, F1: 0.427\n"
     ]
    }
   ],
   "source": [
    "f1_stem_wordvec_cnn = round(f1_score(y_true_classes, y_pred_classes, average='weighted'), 3)\n",
    "accuracy_stem_wordvec_cnn = round(accuracy_score(y_true_classes, y_pred_classes), 3)\n",
    "\n",
    "print(f\"Tikslumas: {accuracy_stem_wordvec_cnn}, F1: {f1_stem_wordvec_cnn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2VEC labai pagerino modelio veikima, siuo atveju tai veike geriau nei visi bandyti CNN, SVC, LSTM su tf-idf vektorizacija."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming + Word2Vec + SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# SVC atveju nereikia encodinti zanru.\n",
    "y = stemming_df['GENRE']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "model_stem_wordvec_svc = SVC()\n",
    "history_stem_wordvec_svc = model_stem_wordvec_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/model_stem_wordvec_svc.joblib']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "dump(model_stem_wordvec_svc, 'models/model_stem_wordvec_svc.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_stem_wordvec_svc = load('models/model_stem_wordvec_svc.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' action ', ' drama ', ' short ', ..., ' drama ', ' documentary ',\n",
       "       ' documentary '], dtype=object)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_stem_wordvec_svc.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tikslumas: 0.568, F1: 0.522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "f1_stem_wordvec_svc = round(f1_score(y_test, y_pred, average='weighted'), 3)\n",
    "accuracy_stem_wordvec_svc = round(accuracy_score(y_test, y_pred), 3)\n",
    "\n",
    "print(f\"Tikslumas: {accuracy_stem_wordvec_svc}, F1: {f1_stem_wordvec_svc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gauti žymiai geresni rezultatai SVC su word2vec atveju nei prieš tai buve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming + Word2Vec + LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = onehot_encoder.fit_transform(stemming_df[\"GENRE\"].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54086, 200) (54086, 27)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape((X.shape[0], 1, X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54086, 1, 200)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, SpatialDropout1D\n",
    "\n",
    "model_stem_wordvec_lstm = Sequential()\n",
    "model_stem_wordvec_lstm.add(SpatialDropout1D(0.2))\n",
    "model_stem_wordvec_lstm.add(LSTM(100, dropout=0.1, recurrent_dropout=0.2))\n",
    "model_stem_wordvec_lstm.add(Dense(27, activation='softmax'))\n",
    "model_stem_wordvec_lstm.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.3781 - loss: 2.1776 - val_accuracy: 0.4661 - val_loss: 1.7712\n",
      "Epoch 2/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4265 - loss: 1.9283 - val_accuracy: 0.4874 - val_loss: 1.7069\n",
      "Epoch 3/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4271 - loss: 1.9071 - val_accuracy: 0.4943 - val_loss: 1.6856\n",
      "Epoch 4/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4271 - loss: 1.8841 - val_accuracy: 0.4766 - val_loss: 1.7042\n",
      "Epoch 5/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4216 - loss: 1.9007 - val_accuracy: 0.4758 - val_loss: 1.6793\n",
      "Epoch 6/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4296 - loss: 1.8683 - val_accuracy: 0.4795 - val_loss: 1.6758\n",
      "Epoch 7/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4310 - loss: 1.8769 - val_accuracy: 0.4785 - val_loss: 1.6612\n",
      "Epoch 8/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4311 - loss: 1.8684 - val_accuracy: 0.4884 - val_loss: 1.6463\n",
      "Epoch 9/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4367 - loss: 1.8618 - val_accuracy: 0.4858 - val_loss: 1.6484\n",
      "Epoch 10/10\n",
      "\u001b[1m1353/1353\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.4364 - loss: 1.8580 - val_accuracy: 0.4880 - val_loss: 1.6457\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "callback = [EarlyStopping(monitor='val_loss', min_delta=0, patience=2, mode='auto')]\n",
    "history_stem_wordvec_lstm = model_stem_wordvec_lstm.fit(X_train, y_train, epochs=10, batch_size=32,validation_data=(X_test,y_test), callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m339/339\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.15853055, 0.00144819, 0.03760834, ..., 0.08100306, 0.01150532,\n",
       "        0.00449546],\n",
       "       [0.00414513, 0.00344206, 0.01010975, ..., 0.00906015, 0.00060203,\n",
       "        0.00050352],\n",
       "       [0.00269915, 0.01384826, 0.00904027, ..., 0.0065204 , 0.00016134,\n",
       "        0.00076548],\n",
       "       ...,\n",
       "       [0.00943357, 0.00837182, 0.0100858 , ..., 0.01022983, 0.00192649,\n",
       "        0.00052781],\n",
       "       [0.10129371, 0.00149402, 0.01660184, ..., 0.01496115, 0.03977218,\n",
       "        0.01162436],\n",
       "       [0.00493572, 0.00029041, 0.0065052 , ..., 0.00361356, 0.00251637,\n",
       "        0.00105068]], dtype=float32)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model_stem_wordvec_lstm.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tikslumas: 0.488, F1: 0.401\n"
     ]
    }
   ],
   "source": [
    "y_pred_classes = y_pred.argmax(axis=1)\n",
    "y_true_classes = y_test.argmax(axis=1) \n",
    "\n",
    "f1_stem_wordvec_lstm = round(f1_score(y_true_classes, y_pred_classes, average='weighted'), 3)\n",
    "accuracy_stem_wordvec_lstm = round(accuracy_score(y_true_classes, y_pred_classes), 3)\n",
    "\n",
    "print(f\"Tikslumas: {accuracy_stem_wordvec_lstm}, F1: {f1_stem_wordvec_lstm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Modelis         Apdorojimas  Tikslumas     F1\n",
      "5     SVC  Word2Vec Stemingas      0.568  0.522\n",
      "4     CNN  Word2Vec Stemingas      0.512  0.427\n",
      "6    LSTM  Word2Vec Stemingas      0.488  0.401\n",
      "2     SVC    TF-IDF Stemingas      0.471  0.402\n",
      "3    LSTM    TF-IDF Stemingas      0.470  0.397\n",
      "0     CNN    TF-IDF Lemavimas      0.443  0.354\n",
      "1     CNN    TF-IDF Stemingas      0.439  0.339\n"
     ]
    }
   ],
   "source": [
    "# Gauti rezultatai\n",
    "import pandas as pd\n",
    "\n",
    "# Create a dictionary to organize the data\n",
    "data = {\n",
    "    'Modelis': ['CNN', 'CNN', 'SVC', 'LSTM', 'CNN', 'SVC', 'LSTM'],\n",
    "    'Apdorojimas': ['TF-IDF Lemavimas', 'TF-IDF Stemingas', 'TF-IDF Stemingas', 'TF-IDF Stemingas', \n",
    "                     'Word2Vec Stemingas', 'Word2Vec Stemingas', 'Word2Vec Stemingas'],\n",
    "    'Tikslumas': [\n",
    "        accuracy_f1_lem_tfidf_cnn,\n",
    "        accuracy_stem_tfidf_cnn,\n",
    "        accuracy_stem_tfidf_svc,\n",
    "        accuracy_stem_tfidf_lstm,\n",
    "        accuracy_stem_wordvec_cnn,\n",
    "        accuracy_stem_wordvec_svc,\n",
    "        accuracy_stem_wordvec_lstm\n",
    "    ],\n",
    "    'F1': [\n",
    "        f1_lem_tfidf_cnn,\n",
    "        f1_stem_tfidf_cnn,\n",
    "        f1_stem_tfidf_svc,\n",
    "        f1_stem_tfidf_lstm,\n",
    "        f1_stem_wordvec_cnn,\n",
    "        f1_stem_wordvec_svc,\n",
    "        f1_stem_wordvec_lstm\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df_sorted = df.sort_values(by='Tikslumas', ascending=False)\n",
    "print(df_sorted)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  Modelis     Apdorojimas       Tikslumas     F1     \n",
    "1.  SVC     Word2Vec Stemingas    0.567     0.520 \n",
    "2.  CNN     Word2Vec Stemingas    0.502     0.411\n",
    "3.  LSTM    Word2Vec Stemingas    0.485     0.401\n",
    "4.  SVC     TF-IDF Stemingas      0.471     0.402\n",
    "5.  LSTM    TF-IDF Stemingas      0.470     0.401\n",
    "6.  CNN     TF-IDF Lemavimas      0.442     0.350\n",
    "7.  CNN     TF-IDF Stemingas      0.439     0.344"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parametru optimizavimo pabandymas, su geriausiu gautu variantu Stemingas + Word2Vec + SVC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "y = stemming_df['GENRE']\n",
    "X = X.reshape(X.shape[0], X.shape[2])\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Geriausi parametrai: {'C': 1, 'class_weight': None, 'kernel': 'rbf'}\n",
      "Geriausias rezultatas: 0.5651751072892826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Modelio optimizavimas taikant gardeles paieska, taciau del dideliu duomenu kiekiu gali trukti kelias valandas.\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],              # Reguliarizacijos parametras\n",
    "    'kernel': ['linear', 'rbf', 'poly'],  # Kernelio tipas\n",
    "    'class_weight': [None, 'balanced']  # Klasiu svoriai\n",
    "}\n",
    "\n",
    "svc_model = SVC()\n",
    "grid_search = GridSearchCV(estimator=svc_model, param_grid=param_grid, \n",
    "                           cv=5,  # Number of folds in cross-validation\n",
    "                           scoring='accuracy',  # Metric to evaluate the performance\n",
    "                           n_jobs=-1,  # Naudoti visus imanomus procesorius\n",
    "                           verbose=1)  # Spausdinamos info kiekis\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Geriausi parametrai: {grid_search.best_params_}\")\n",
    "print(f\"Geriausias rezultatas: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kodel modelis liko toks pat/nepagerejo po optimizacijos? Nes geriausius parametrus, kuriuos jis surado - ir buvo nutyleti pagrindiniai SVC parametrai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Išvados:\n",
    "1. Geriausiai pasirodė Stemingo + Word2Vec + SVC derinys su 57% tikslumu ir didžiausiu F1 rodikliu. Tai atrodo ne tik kaip geras rezultatas, atsižvelgiant kad turėjome 27 klases tarp kurių prognozuota. Šis modelis ne tik pasiekė didžiausia tikslumą, bet ir geriausias F1 rodo, kad jis puikiai suderino klasių dydžių disbalansą.\n",
    "2. Visais atvejais Word2Vec vektorizavimo būdas buvo ženkliai pranašesnis už tf-idf vektorius.\n",
    "3. Stemingas ir Lemavimas suteikia panašius rezultatus, tad jei taupomas laikas, kartais neverta visų kombinacijų išbandyti. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rekomendacijos tobulinimui:\n",
    "1. Daugiau privengti pasikartojančio kodo iškeliant jį į funkcijas.\n",
    "2. Sugalvoti tinkamesnį užvadinimą train/test data pasikartojantiems kintamiesiems, nes labai maišosi.\n",
    "3. Galima pabandyti pakeitinėti vektorizavimo technikų parametrus.\n",
    "4. Galima išbandyti, gal kai kurie variantai geriau su lematizavimu veiktu.\n",
    "5. Realaus pasaulio duomenims reiktu sukurti patogia funkcija/klase kuria galima butu juos taip pat apdoroti, kaip ir mokymosi duomenis (pvz.: steminguoti, vektorizuoti), prieš taikant modeli. \n",
    "6. Naudoti seed()/random_state pythone, tam kad išlaikyti tuos pačius modelio gautus rezultus (šiuo atveju per vėlai suprasta ir panaudota ne visur).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
